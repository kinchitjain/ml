{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you need to modify the Viterbi algorithm to solve the problem of unknown words using **at least two techniques**. Though there could be multiple ways to solve this problem, you may use the following hints:\n",
    "\n",
    "1. Which tag class do you think most unknown words belong to? Can you identify rules (e.g. based on morphological cues) that can be used to tag unknown words? You may define separate python functions to exploit these rules so that they work in tandem with the original Viterbi algorithm.\n",
    "2. Why does the Viterbi algorithm choose a random tag on encountering an unknown word? Can you modify the Viterbi algorithm so that it considers only one of the transition or emission probabilities for unknown words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split the Treebank dataset into train and validation sets. Please **use a sample size of 95:5** for training: validation sets, i.e. keep the validation size small, else the algorithm will need a very high amount of runtime.\n",
    "\n",
    "You need to accomplish the following in this assignment:\n",
    "* Write the vanilla Viterbi algorithm for assigning POS tags (i.e. without dealing with unknown words) \n",
    "* Solve the problem of unknown words using **at least two techniques**. These techniques can use any of the approaches discussed in the class - lexicon, rule-based, probabilistic etc. Note that to implement these techniques, you can either write separate functions and call them from the main Viterbi algorithm, or modify the Viterbi algorithm, or both.\n",
    "* Compare the tagging accuracy after making these modifications with the vanilla Viterbi algorithm.\n",
    "* List down at least three cases from the sample test file (i.e. unknown word-tag pairs) which were incorrectly tagged by the original Viterbi POS tagger and got corrected after your modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check some of the tagged data\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation set in the ratio 95:5\n",
    "random.seed(1234)\n",
    "train_set, test_set = train_test_split(data, train_size=0.95, test_size=0.05)\n",
    "\n",
    "print(\"Training Set Length -\", len(train_set))\n",
    "print(\"Testing Set Length -\", len(test_set))\n",
    "print(\"-\" * 100)\n",
    "print(\"Training Data -\\n\")\n",
    "print(train_set[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of train and test tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "print(\"Train Tagged Words - \", len(train_tagged_words))\n",
    "\n",
    "test_tagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "print(\"Train Tagged Words - \", len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the tagged words in the training set\n",
    "train_tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens in the train set - train_tagged_words\n",
    "train_tagged_tokens = [tag[0] for tag in train_tagged_words]\n",
    "train_tagged_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags for the tokens in the train set - train_tagged_words\n",
    "\n",
    "train_tagged_pos_tokens = [tag[1] for tag in train_tagged_words]\n",
    "train_tagged_pos_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the train vocabulary to a set\n",
    "training_vocabulary_set = set(train_tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the POS tags to a set\n",
    "training_pos_tag_set = set(train_tagged_pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how many unique tags are present in training data\n",
    "print(len(training_pos_tag_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how many words are present in vocabulary\n",
    "print(len(training_vocabulary_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute emission probabilties for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute emission probability for a given word for a given tag\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1] == tag]\n",
    "    tag_count = len(tag_list)    \n",
    "    word_given_tag_list = [pair[0] for pair in tag_list if pair[0] == word]    \n",
    "    word_given_tag_count = len(word_given_tag_list)    \n",
    "    \n",
    "    return (word_given_tag_count, tag_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute transition probabilties for a given tag and previous tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute transition probabilities of a previous and next tag\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    \n",
    "    t1_tags_list = [tag for tag in tags if tag == t1]\n",
    "    t1_tags_count = len(t1_tags_list)\n",
    "    \n",
    "    t2_given_t1_list = [tags[index+1] for index in range(len(tags)-1) if tags[index] == t1 and tags[index+1] == t2]\n",
    "    t2_given_t1_count = len(t2_given_t1_list)\n",
    "    \n",
    "    return(t2_given_t1_count, t1_tags_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in [Tags x Vocabulary] matrix. This is a matrix with dimension\n",
    "# of len(training_pos_tag_set) X en(training_vocabulary_set)\n",
    "\n",
    "len_pos_tags = len(training_pos_tag_set)\n",
    "len_vocab = len(training_vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of training_pos_tag_set\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len_pos_tags, len_pos_tags), dtype='float32')\n",
    "for i, t1 in enumerate(list(training_pos_tag_set)):\n",
    "    for j, t2 in enumerate(list(training_pos_tag_set)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(training_pos_tag_set), index=list(training_pos_tag_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a glimpse into the transition matrix\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of tags matrix\n",
    "# T(i, j) means P(tag j given tag i)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(tags_df, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent tags\n",
    "# filter the df to get P(t2, t1) > 0.5\n",
    "tags_frequent = tags_df[tags_df>0.5]\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(tags_frequent, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Given a sequence of words.\n",
    "2. iterate through the sequence\n",
    "3. for each word (starting from first word in sequence) calculate the product of emission probabilties and transition probabilties for all possible tags.\n",
    "4. assign the tag which has maximum probability obtained in step 3 above.\n",
    "5. move to the next word in sequence to repeat steps 3 and 4 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Viterbi Algorithm\n",
    "def Vanilla_Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vanilla Viterbi Algorithm on sampled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1, len(test_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Vanilla_Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "vanilla_viterbi_word_check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "vanilla_viterbi_accuracy = len(vanilla_viterbi_word_check)/len(tagged_seq) * 100\n",
    "print('Vanilla Viterbi Algorithm Accuracy: ', vanilla_viterbi_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the incorrectly tagged words\n",
    "incorrect_tagged_words = [j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0] != j[1]]\n",
    "\n",
    "print(\"Total Incorrect Tagged Words :\", len(incorrect_tagged_words))\n",
    "print(\"\\n\")\n",
    "print(\"Incorrect Tagged Words :\", incorrect_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding for dealing with unknown words, lets first see how many unknown words we have. Unknows words would be\n",
    "those words that are present in the test set but not in the train set. That is the words the algorithm has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown words \n",
    "\n",
    "test_vocabulary_set = set([t for t in test_tagged_words])\n",
    "\n",
    "unknown_words = list(test_vocabulary_set - training_vocabulary_set)\n",
    "print(\"Total Unknown words :\", len(unknown_words))\n",
    "print(\"\\n\")\n",
    "print(\"Unknown Words :\", unknown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try with some Lexicon and Rule-Based Models for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon (or unigram tagger)\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_set)\n",
    "accuracy_unigram_tagger = unigram_tagger.evaluate(test_set)\n",
    "print(\"The accuracy of the Unigram Tagger is -\", accuracy_unigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try combining the unigram tagger with a rule based regex tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns for tagging using a rule based regex tagger -\n",
    "\n",
    "patterns = [\n",
    "    (r'^[aA-zZ].*[0-9]+','NOUN'),  # Alpha Numeric\n",
    "    (r'.*ness$', 'NOUN'),\n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'.*', 'NOUN'),    \n",
    "    (r'.*ly$', 'ADV'),\n",
    "    (r'^(0|([*|-|$].*))','X'), # Any special character combination\n",
    "    (r'.*ould$', 'X'), # modals\n",
    "    (r'(The|the|A|a|An|an)$', 'DET'),\n",
    "    (r'^([0-9]|[aA-zZ])+\\-[aA-zZ]*$','ADJ'),\n",
    "    (r'.*able$', 'ADJ'), # adjective like 100-megabytes 237-Seats\n",
    "    (r'[aA-zZ]+(ed|ing|es)$', 'VERB'), # Any word ending with 'ing' or 'ed' is a verb\n",
    "    (r'[0-9].?[,\\/]?[0-9]*','NUM')# Numbers \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based tagger\n",
    "\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "# unigram tagger backed up by the rule-based tagger\n",
    "rule_based_unigram_tagger = nltk.UnigramTagger(train_set, backoff = rule_based_tagger)\n",
    "\n",
    "accuracy_rule_based_unigram_tagger = rule_based_unigram_tagger.evaluate(test_set)\n",
    "\n",
    "print(\"The accuracy of the Unigram Tagger backed up by the RegexpTagger is -\", accuracy_rule_based_unigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram and Trigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram tagger\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_set, backoff=rule_based_unigram_tagger)\n",
    "bigram_tagger.evaluate(test_set)\n",
    "accuracy_bigram_tagger = bigram_tagger.evaluate(test_set)\n",
    "print(accuracy_bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram tagger\n",
    "\n",
    "trigram_tagger = nltk.TrigramTagger(train_set, backoff = bigram_tagger)\n",
    "trigram_tagger.evaluate(test_set)\n",
    "accuracy_trigram_tagger = trigram_tagger.evaluate(test_set)\n",
    "print(\"The accuracy of the Trigram Tagger backed up by the bigram_tagger is -\", accuracy_trigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Trigram Tagger backed up by the bigram tagger gives an accuracy of about 93.87%. Let's now try to modify the viterbi algorithm to use this trigram tagger as a back-off.\n",
    "\n",
    "When the viterbi algorithm is not able to tag an unknown word, it uses the rule-based tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification-Technique I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use transition probability of tags when emission probability is zero (in case of unknown words)\n",
    "\n",
    "def Vanilla_Viterbi_for_Unknown_Words(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        p_transition =[] # list for storing transition probabilities\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            p_transition.append(transition_p)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = T[p.index(pmax)] \n",
    "        \n",
    "      \n",
    "        # if probability is zero (unknown word) then use transition probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(p_transition)\n",
    "            state_max = T[p_transition.index(pmax)]\n",
    "                           \n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Vanilla_Viterbi_for_Unknown_Words(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Vanilla Viterbi for Unknown Words Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Tag occurance probability weights:** we will apply weights based on the probability of tag occurance to the transition probabilities of tags and then use the resulting probability for predicting unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a list containing tuples of POS tags and POS tag occurance probability, based on training data\n",
    "tag_prob = []\n",
    "total_tag = len([tag for word,tag in train_tagged_words])\n",
    "for t in training_pos_tag_set:\n",
    "    each_tag = [tag for word,tag in train_tagged_words if tag==t]\n",
    "    tag_prob.append((t,len(each_tag)/total_tag))\n",
    "\n",
    "tag_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use transition probability of tags when emission probability is zero (in case of unknown words)\n",
    "\n",
    "def Vanilla_Viterbi_for_Unknown_Words_Modified(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        p_transition =[] # list for storing transition probabilities\n",
    "       \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "            # find POS tag occurance probability\n",
    "            tag_p = [pair[1] for pair in tag_prob if pair[0]==tag ]\n",
    "            \n",
    "            # calculate the transition prob weighted by tag occurance probability.\n",
    "            transition_p = tag_p[0]*transition_p             \n",
    "            p_transition.append(transition_p)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = T[p.index(pmax)] \n",
    "        \n",
    "      \n",
    "        # if probability is zero (unknown word) then use weighted transition probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(p_transition)\n",
    "            state_max = T[p_transition.index(pmax)]                 \n",
    "                           \n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Vanilla_Viterbi_for_Unknown_Words_Modified(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "viterbi_word_check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy_viterbi_modified = len(viterbi_word_check)/len(tagged_seq) * 100\n",
    "print('Modified Vanilla Viterbi for Unknown Words Accuracy: ', accuracy_viterbi_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that we have got a much better accuracy by using weighted transition probabilties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification-Technique II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trigram tagger backed off by a rule based tagger.\n",
    "\n",
    "def trigram_tagger(word, train_set = train_set):\n",
    "    \n",
    "    patterns = [\n",
    "    (r'[aA-zZ]+(ed|ing|es)$', 'VERB'), # Any word ending with 'ing' or 'ed' is a verb\n",
    "\n",
    "    (r'.*ly$', 'ADV'),\n",
    "        \n",
    "    (r'^([0-9]|[aA-zZ])+\\-[aA-zZ]*$','ADJ'),\n",
    "    (r'.*able$', 'ADJ'), \n",
    "    (r'.*ful$', 'ADJ'),\n",
    "    (r'.*ous$', 'ADJ'),\n",
    "        \n",
    "    (r'^[aA-zZ].*[0-9]+','NOUN'),     # Alpha Numeric\n",
    "    (r'.*ness$', 'NOUN'),\n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns - words ending with 's\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'.*ers$', 'NOUN'),              # eg.- kinderganteners, autobioghapgers\n",
    "    (r'.*ment$', 'NOUN'),\n",
    "    (r'.*town$', 'NOUN'),\n",
    "        \n",
    "    (r'^(0|([*|-|$].*))','X'), # Any special character combination\n",
    "    (r'.*ould$', 'X'),\n",
    "        \n",
    "    (r'(The|the|A|a|An|an|That|that|This|this|Those|those|These|these)$', 'DET'), # That/this/these/those belong to the category of Demonstrative determiners\n",
    "    (r'[0-9].?[,\\/]?[0-9]*','NUM'), # Numbers \n",
    "        \n",
    "    (r'.*', 'NOUN')\n",
    "    ]\n",
    "\n",
    "    regex_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "    # trigram backed up by the regex tagger\n",
    "    trigram_regex_tagger = nltk.TrigramTagger(train_set, backoff = regex_based_tagger)\n",
    "    return trigram_regex_tagger.tag_sents([[(word)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viterbi with handling for unknown words from regex tagger\n",
    "\n",
    "def Viterbi_Trigram_Tagger(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        # unknown words from trigram taggr\n",
    "        if word not in training_vocabulary_set:\n",
    "            unk_word_tag=trigram_tagger(word)\n",
    "            for sent in unk_word_tag:\n",
    "                for tup in sent:\n",
    "                    state.append(tup[1])\n",
    "        # rest remains same            \n",
    "        else:            \n",
    "            p = [] \n",
    "            for tag in T:\n",
    "                if key == 0:\n",
    "                    transition_p = tags_df.loc['.', tag]\n",
    "                else:\n",
    "                    transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "                emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "                state_probability = emission_p * transition_p    \n",
    "                p.append(state_probability)\n",
    "            \n",
    "            pmax = max(p)\n",
    "            # getting state for which probability is maximum\n",
    "            state_max = T[p.index(pmax)] \n",
    "            state.append(state_max)\n",
    "            \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi_Trigram_Tagger(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "viterbi_trigram_word_check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "viterbi_trigram_accuracy = len(viterbi_trigram_word_check)/len(tagged_seq) * 100\n",
    "print('Modified Viterbi Trigram Tagger Accuracy: ', viterbi_trigram_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that much better accuracy is obtained now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acccuracy_data = [['Vanilla Viterbi', vanilla_viterbi_accuracy], \n",
    "                  ['Vanilla Viterbi Modified', accuracy_viterbi_modified], \n",
    "                  ['Unigram Tagger', accuracy_unigram_tagger * 100],\n",
    "                  ['Unigram + RegexpTagger', accuracy_rule_based_unigram_tagger * 100],\n",
    "                  ['Bigram Tagger + Unigram_tagger', accuracy_bigram_tagger*100],\n",
    "                  ['Trigram Tagger + Bigram_tagger', accuracy_trigram_tagger*100],\n",
    "                  ['Viterbi + Trigram_tagger', viterbi_trigram_accuracy]]\n",
    "\n",
    "acccuracy_data_df = pd.DataFrame(acccuracy_data, columns = ['Tagging_Algorithm', 'Tagging_Accuracy'])\n",
    "\n",
    "acccuracy_data_df.set_index('Tagging_Algorithm', drop = True, inplace = True)\n",
    "\n",
    "acccuracy_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acccuracy_data_df.plot.line(rot = 90, legend = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "sentence_test_1 = 'Google and Twitter made a deal in 2015 that gave Google access to Twitter\\'s firehose.'\n",
    "words = word_tokenize(sentence_test_1)\n",
    "tagged_seq = Vanilla_Viterbi(words)\n",
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_seq_modified = Viterbi_Trigram_Tagger(words)\n",
    "print(tagged_seq_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_test_2='Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.'\n",
    "words = word_tokenize(sentence_test_2)\n",
    "tagged_seq = Vanilla_Viterbi(words)\n",
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_seq_modified = Viterbi_Trigram_Tagger(words)\n",
    "print(tagged_seq_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_test_3='I Instagrammed a Facebook post taken from Android smartphone and uploaded results to Youtube.'\n",
    "words = word_tokenize(sentence_test_3)\n",
    "tagged_seq = Vanilla_Viterbi(words)\n",
    "print(tagged_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_seq_modified = Viterbi_Trigram_Tagger(words)\n",
    "print(tagged_seq_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the new words - Youtube, Facebook, smartphone going from X to NOUN. We alse see the verb of Instagrammed,uploaded getting recognised correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
